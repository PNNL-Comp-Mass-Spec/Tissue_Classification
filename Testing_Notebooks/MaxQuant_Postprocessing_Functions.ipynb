{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Processing of MaxQuant output (proteinGroups.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy import logical_or\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: Path to tab-sep text document\n",
    "Output: Pandas dataframe\n",
    "\"\"\"\n",
    "def load_df(file):\n",
    "    df = pd.read_csv(file, sep='\\t', lineterminator='\\r', dtype={\"Only identified by site\": str, \"Reverse\": str, \"Potential contaminant\": str})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data\n",
    "* Remove rows corresponding to the proteins only identified by site/Reverse/Potential contaminant\n",
    "* Remove rows with multiple protein IDs\n",
    "* Extract separately the LFQ and the iBAQ quantification info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: Dataframe (assumes columns 'Only identified by site', 'Reverse', 'Potential contaminant')\n",
    "Output: Dataframe with rows having a '+' in any of these columns removed\n",
    "\"\"\"\n",
    "def clean_weakly_identified(df):\n",
    "    df = df[(df['Only identified by site'] != '+') & (df.Reverse != '+') & (df['Potential contaminant'] != '+')]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: Dataframe\n",
    "Output: Dataframe where rows containing multiple protein IDs have been removed\n",
    "\"\"\"\n",
    "def remove_dup_proteinIDs(df):\n",
    "    single_proteinID = df['Majority protein IDs'].str.contains(';') == False\n",
    "    df = df[single_proteinID]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: Dataframe, string\n",
    "Output: Dataframe filtered to contain the protein ID column and columns containing the input string\n",
    "\"\"\"\n",
    "\n",
    "def slice_by_column(df, col_name):\n",
    "    selected_col_name = col_name + \".*|Majority protein IDs\"\n",
    "    df_slice = df.filter(regex = selected_col_name)\n",
    "    return df_slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize\n",
    "### For both LFQ and iBAQ:\n",
    "* Consider only the proteins observed at least in 50 percent of the sample for at least one organ for quantification\n",
    "* log2 normalize\n",
    "* Median normalize: the median of the log2(LFQ or IBAQ) of each protein in a given sample is used to normalize all the protein abundance of this sample, then multiply all the resulting values by the median of the medians\n",
    "* Impute the missing values: the minimum of the resulting table divided by 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "# Filter out proteins where quant value is 0 for >= 50% of samples for all organs\n",
    "#\n",
    "#########################\n",
    "\"\"\"\n",
    "Input: dataframe, list of names of groups (as strings) by which to sort column names\n",
    "Output: filtered dataframe\n",
    "\"\"\"\n",
    "def filter_low_observed(df, groups, organ_columns, organ_counts):\n",
    "    samples_per_group = 6  # TODO dynamically assign variable\n",
    "    threshold = samples_per_group/2\n",
    "    df_cols = df.columns.values.tolist()\n",
    "    \n",
    "    for group in groups:\n",
    "        regex = re.compile(r'.*' + group)\n",
    "        organ_columns[group] = list(filter(regex.search, df_cols))\n",
    "        cols = organ_columns[group] # Get corresponding list of column names\n",
    "        organ_counts[group] = (df[cols] > 0).sum(1) # count number of samples with non-zero abundance for each protein\n",
    "        \n",
    "    conditions = list(organ_counts[g] >= threshold for g in groups)\n",
    "    df = df[logical_or.reduce(conditions)]\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "# Unnormalized data abundances \n",
    "#\n",
    "#########################\n",
    "\n",
    "### TODO: dynamically order columns\n",
    "# Group columns by organ so x-axis will be sorted accordingly\n",
    "#iBAQ_df = iBAQ_df[['Majority protein IDs'] + organ_columns['Brain'] + organ_columns['Heart'] + organ_columns['Kidney'] + organ_columns['Liver'] + organ_columns['Lung']]\n",
    "\n",
    "def make_boxplot(df, title, dimensions = (10, 6)):\n",
    "    df.boxplot(return_type='axes', figsize = dimensions)\n",
    "    plt.xticks(rotation='vertical')\n",
    "    output_path = base_dir + title + '.pdf'\n",
    "    plt.savefig(output_path, bbox_inches=\"tight\")\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "# log2 normalize\n",
    "#\n",
    "#########################\n",
    "\n",
    "def log2_normalize(df):\n",
    "    df.iloc[:,1:] = df.iloc[:,1:].applymap(np.log2)\n",
    "    # log2(0) returns -inf; replace with NaN to avoid skewing data\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "# Map organs to colors for visualization clarity \n",
    "#\n",
    "#########################\n",
    "\"\"\"\n",
    "Input: List of strings, dict\n",
    "Output: Dict mapping columns to colors based on organ/group\n",
    "\"\"\"\n",
    "def map_colors(groups, organ_columns):\n",
    "    color_dict = {} # Column name : color\n",
    "    num_colors = 6\n",
    "    colors = sns.color_palette(\"hls\", num_colors)\n",
    "    color = 0\n",
    "\n",
    "    for organ in groups:\n",
    "        cols = organ_columns[organ] # Get the list of column names for the organ\n",
    "        for col in cols:\n",
    "            color_dict[col] = colors[color % len(colors)]\n",
    "        color += 1\n",
    "        \n",
    "    return color_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_seaborn_boxplot(df, title, colors, dimensions = (10, 6)):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = dimensions)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\n",
    "    sns.boxplot(data = df, palette = colors, ax = ax)\n",
    "    output_path = base_dir + title + '.pdf'\n",
    "    plt.savefig(output_path, bbox_inches = \"tight\")\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "# Median normalize\n",
    "#\n",
    "#########################\n",
    "\n",
    "def median_normalize(df):\n",
    "    quants = df.iloc[:,1:] # Split off iBAQ columns to process\n",
    "    median_of_medians = quants.median().median()\n",
    "    quants /= quants.median(axis = 0) # divide each value by sample median\n",
    "    quants *= median_of_medians # multiply each value by median of medians\n",
    "\n",
    "    df.iloc[:,1:] = quants # insert processed iBAQ values into original df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "# Impute missing values\n",
    "#\n",
    "#########################\n",
    "\n",
    "def impute_missing(df):\n",
    "    df_min = df.iloc[:,1:].min().min()\n",
    "    impute_val = df_min/2\n",
    "    df = df.fillna(impute_val)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "# Perform PCA on the data\n",
    "#\n",
    "#########################\n",
    "\n",
    "\"\"\"\n",
    "Input: unindexed dataframe (first column will become index)\n",
    "Output: tuple (PCA object, PCA coordinates for dataframe)\n",
    "\"\"\"\n",
    "def do_pca(df):\n",
    "\n",
    "    df.set_index('Majority protein IDs', inplace=True)\n",
    "    scaled_data = preprocessing.scale(df.T)\n",
    "\n",
    "    pca = PCA() # create a PCA object\n",
    "    pca.fit(scaled_data) # do the math\n",
    "    pca_data = pca.transform(scaled_data) # get PCA coordinates for dataframe\n",
    "    \n",
    "    return(pca, pca_data)\n",
    "    \n",
    "#########################\n",
    "#\n",
    "# Draw a scree plot \n",
    "#\n",
    "#########################\n",
    "\n",
    "def make_scree_plot(pca, pca_data, title):\n",
    "\n",
    "    per_var = np.round(pca.explained_variance_ratio_* 100, decimals = 1)\n",
    "    labels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n",
    " \n",
    "    plt.bar(x = range(1, len(per_var) + 1), height = per_var, tick_label = labels)\n",
    "    plt.ylabel('Percentage of Explained Variance')\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.title('Scree Plot')\n",
    "    plt.xticks(rotation='vertical')\n",
    "    output_path = base_dir + title + '.pdf'\n",
    "\n",
    "    plt.savefig(title, bbox_inches=\"tight\")\n",
    "    plt.clf()\n",
    "    \n",
    "    return(per_var, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "# Draw PCA Graph \n",
    "#\n",
    "#########################\n",
    "\n",
    "def draw_pca_graph(df, pca_data, title, color_dict, per_var, labels):\n",
    "    \n",
    "    pca_df = pd.DataFrame(pca_data, index = [df.columns.values.tolist()], columns = labels)\n",
    " \n",
    "    plt.title('PCA Graph')\n",
    "    plt.xlabel('PC1 - {0}%'.format(per_var[0]))\n",
    "    plt.ylabel('PC2 - {0}%'.format(per_var[1]))\n",
    " \n",
    "    for column in pca_df.index:\n",
    "        plt.scatter(pca_df.PC1.loc[column], pca_df.PC2.loc[column], color = color_dict[column])\n",
    "        plt.annotate(column, (pca_df.PC1.loc[column], pca_df.PC2.loc[column]), color = color_dict[column])\n",
    "\n",
    "    output_path = base_dir + title + '.pdf'\n",
    "    plt.savefig(output_path, bbox_inches=\"tight\")\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "# Determine which proteins had the biggest influence on PC1 \n",
    "#\n",
    "#########################\n",
    "\n",
    "def top_n_loading_scores(pca, df, n):\n",
    "    \n",
    "    loading_scores = pd.Series(pca.components_[0], index = df.index)\n",
    "    sorted_loading_scores = loading_scores.abs().sort_values(ascending = False)\n",
    "\n",
    "    top_proteins = sorted_loading_scores[0:n].index.values\n",
    "    return loading_scores[top_proteins]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmaps\n",
    "* Pearson Correlations\n",
    "* Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "# Pearson correlation of the samples compared to each other \n",
    "#\n",
    "#########################\n",
    "\n",
    "def make_pearson_matrix(df, title, colormap = \"RdBu_r\", dimensions = (16, 11)):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = dimensions)\n",
    "    ax.set_title('Pearson Correlations', size = 20)\n",
    "\n",
    "    corr = df.corr(method = 'pearson')\n",
    "    sns.heatmap(corr, \n",
    "                xticklabels = corr.columns.values,\n",
    "                yticklabels = corr.columns.values,\n",
    "                annot = True, # Show numerical values in each box\n",
    "                cmap = colormap, \n",
    "                ax = ax) \n",
    "    \n",
    "    output_path = base_dir + title + '.pdf'\n",
    "    plt.savefig(output_path, bbox_inches=\"tight\")\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "# Hierarchical clustering of proteins\n",
    "#\n",
    "#########################\n",
    "\n",
    "def hierarchical_cluster(df, title, dimensions = (10, 6)):\n",
    "\n",
    "    z = linkage(df.values, method='ward')\n",
    "\n",
    "    plt.figure(figsize = dimensions)\n",
    "    plt.title('Hierarchical Clustering of Proteins')\n",
    "    plt.ylabel('distance')\n",
    "    dendrogram(z,\n",
    "               leaf_rotation=90.,  # rotates the x axis labels\n",
    "               #leaf_font_size=8.,  # font size for the x axis labels\n",
    "              )\n",
    "    \n",
    "    #output_path = base_dir + title + '.pdf'\n",
    "    plt.savefig(title, bbox_inches=\"tight\")\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA and t-tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_proteins_by_anova(df, pval):\n",
    "    # Build list of proteins that pass ANOVA\n",
    "    pass_anova = []\n",
    "    max_pval = pval\n",
    "    proteins = list(df.index)\n",
    "\n",
    "    # Perform ANOVA on each row (protein) grouping by organ\n",
    "    # If the protein passes ANOVA (p-value <= max_pval), add it to the list of proteins to keep\n",
    "    for i in range(len(df)): \n",
    "        f, p = stats.f_oneway(df.iloc[i, :6],\n",
    "                              df.iloc[i, 6:12],\n",
    "                              df.iloc[i, 12:18], \n",
    "                              df.iloc[i, 18:24], \n",
    "                              df.iloc[i, 24:30])\n",
    "        if p <= max_pval:\n",
    "            pass_anova.append(proteins[i])\n",
    "\n",
    "    # Filter dataframe down to only include proteins in pass_anova\n",
    "    pass_anova_df = df[df.index.isin(pass_anova)]\n",
    "    return pass_anova_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "# Heatmap of proteins\n",
    "#\n",
    "#########################\n",
    "\n",
    "def protein_heatmap(df, title, colormap = \"RdBu_r\"):\n",
    "\n",
    "    sns.clustermap(df,\n",
    "                   method = 'ward',\n",
    "                   z_score = 1, # on columns\n",
    "                   cmap = colormap)\n",
    "\n",
    "    output_path = base_dir + title + '.pdf'\n",
    "    plt.savefig(output_path, bbox_inches = \"tight\")\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "# Tukey Test\n",
    "#\n",
    "#########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-51-dbf3ea841d6b>, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-51-dbf3ea841d6b>\"\u001b[1;36m, line \u001b[1;32m23\u001b[0m\n\u001b[1;33m    print(sorted_df.columns.values.tolist())\u001b[0m\n\u001b[1;37m                                            \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def reorder_columns(df, organs, organ_to_columns):\n",
    "    all_cols = list(organ_columns[o] for o in organs)\n",
    "    merged = list(itertools.chain.from_iterable(all_cols))\n",
    "    df = df[['Majority protein IDs'] + merged]\n",
    "    return df\n",
    "\n",
    "\"\"\"\n",
    "Testing:\n",
    "datafile = \"D:\\proteinGroupsCLeaned.txt\"\n",
    "df = load_df(datafile)\n",
    "iBAQ_df = slice_by_column(df, 'iBAQ ') \n",
    "\n",
    "orgs = ['Brain', 'Heart', 'Kidney', 'Liver', 'Lung']\n",
    "\n",
    "organ_columns = {} # 'Liver': ['iBAQ 04_Liver', 'iBAQ 05_Liver', ...]\n",
    "organ_counts = {} # 'Liver': \n",
    "iBAQ_df = filter_low_observed(iBAQ_df, orgs, organ_columns, organ_counts)\n",
    "\n",
    "sorted_df = reorder_columns(iBAQ_df, orgs, organ_columns)\n",
    "#sorted_df = iBAQ_df.reindex_axis(sorted(iBAQ_df.columns), axis=1)\n",
    "print(sorted_df.columns.values.tolist())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "# Full Pipeline \n",
    "#\n",
    "#########################\n",
    "\"\"\"\n",
    "Runs a spreadsheet through the process of cleaning and analyzing, producing charts\n",
    "\n",
    "Input: path to proteinGroupt.txt file, list of group names (e.g. ['Brain', 'Lung' ...]), directory for images\n",
    "Output: Log2 and median normalized dataframe (missing values not imputed). Images will be saved into image_dir\n",
    "\"\"\"\n",
    "def mq_pipeline(file, groups, image_dir):\n",
    "    df = load_df(file)\n",
    "    df = clean_weakly_identified(df)\n",
    "    df = remove_dup_proteinIDs(df)\n",
    "        \n",
    "    iBAQ_df = slice_by_column(df, 'iBAQ ') \n",
    "    #LFQ_df = slice_by_column(df, 'LFQ') \n",
    "    \n",
    "    organ_columns = {} # 'Liver': ['iBAQ 04_Liver', 'iBAQ 05_Liver', ...]\n",
    "    organ_counts = {} # 'Liver': \n",
    "    \n",
    "    iBAQ_df = filter_low_observed(iBAQ_df, groups, organ_columns, organ_counts)\n",
    "    make_boxplot(iBAQ_df, 'Unnormalized Protein Abundances')\n",
    "    \n",
    "    # Group columns by organ so x-axis will be sorted accordingly\n",
    "    iBAQ_df = reorder_columns(iBAQ_df, groups, organ_columns)\n",
    "    \n",
    "    ### Normalize and produce box plots\n",
    "    log2_normalize(iBAQ_df)\n",
    "    color_dict = map_colors(groups, organ_columns)\n",
    "    make_seaborn_boxplot(iBAQ_df, 'Log2 Transformed Boxplot', color_dict)\n",
    "    median_normalize(iBAQ_df)\n",
    "    make_seaborn_boxplot(iBAQ_df, 'Median Normalized Boxplot', color_dict)\n",
    "    \n",
    "    ### PCA\n",
    "    imputed_iBAQ_df = impute_missing(iBAQ_df.copy())\n",
    "    pca, pca_data = do_pca(imputed_iBAQ_df)\n",
    "    \n",
    "    per_var, labels = make_scree_plot(pca, pca_data, 'Scree Plot') \n",
    "    draw_pca_graph(imputed_iBAQ_df, pca_data, 'PCA Graph', color_dict, per_var, labels)\n",
    "    make_pearson_matrix(imputed_iBAQ_df, 'Pearson Correlations')\n",
    "    hierarchical_cluster(imputed_iBAQ_df, 'Hierarchical Clustering')\n",
    "    \n",
    "    pval = 0.05\n",
    "    pass_anova_df = filter_proteins_by_anova(imputed_iBAQ_df, pval)\n",
    "    protein_heatmap(pass_anova_df, 'Proteins Passing ANOVA Heatmap')\n",
    "    \n",
    "    return iBAQ_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Example usage\n",
    "\n",
    "datafile = \"D:\\proteinGroupsCLeaned.txt\"\n",
    "peptide_file = \"D:\\peptides.txt\"\n",
    "sample_groups = ['Brain', 'Heart', 'Kidney', 'Liver', 'Lung']\n",
    "base_dir = 'D:\\\\Images\\\\PipelineTest\\\\'\n",
    "\n",
    "#df = mq_pipeline(datafile, sample_groups, base_dir, 'iBAQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kush494\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4451: RuntimeWarning: divide by zero encountered in log2\n",
      "  return lib.map_infer(x.asobject, func)\n"
     ]
    }
   ],
   "source": [
    "import MaxQuant_Postprocessing_Functions as mq\n",
    "\n",
    "df = mq.mq_pipeline(datafile, sample_groups, base_dir, 'iBAQ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO \n",
    "* Write images out to file - take argument for specific dataset name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
